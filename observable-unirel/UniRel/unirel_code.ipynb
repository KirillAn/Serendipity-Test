{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de5612fa-0019-442b-b55c-cb56ab806ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kirillanosov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataprocess'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (BertTokenizerFast)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#import dataprocess.rel2text\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rel2text\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  UniRelModel\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataprocess\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_extractor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataprocess'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import (BertTokenizerFast)\n",
    "#import dataprocess.rel2text\n",
    "from dataprocess import rel2text\n",
    "from model.model_transformers import  UniRelModel\n",
    "from dataprocess.data_extractor import *\n",
    "from dataprocess.data_metric import *\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "181a5345-d3bf-4bf0-b102-2bd39a93a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniRel:\n",
    "    def __init__(self, model_path, max_length=128, dataset_name=\"nyt\") -> None:\n",
    "        self.model = UniRelModel.from_pretrained(model_path)\n",
    "        added_token = [f\"[unused{i}]\" for i in range(1, 17)]\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(\n",
    "            \"bert-base-cased\", additional_special_tokens=added_token, do_basic_tokenize=False)\n",
    "        self.max_length = max_length\n",
    "        self.max_length = max_length\n",
    "        self._get_pred_str(dataset_name)\n",
    "        \n",
    "    \n",
    "    def _get_pred_str(self, dataset_name):\n",
    "        self.pred2text = None\n",
    "        if dataset_name == \"nyt\":\n",
    "            self.pred2text=dataprocess.rel2text.nyt_rel2text\n",
    "        elif dataset_name == \"nyt_star\":\n",
    "            self.pred2text=dataprocess.rel2text.nyt_rel2text\n",
    "        elif dataset_name == \"webnlg\":\n",
    "            self.pred2text=dataprocess.rel2text.webnlg_rel2text\n",
    "            cnt = 1\n",
    "            exist_value=[]\n",
    "            # Some hard to convert relation directly use [unused]\n",
    "            for k in self.pred2text:\n",
    "                v = self.pred2text[k]\n",
    "                if isinstance(v, int):\n",
    "                    self.pred2text[k] = f\"[unused{cnt}]\" \n",
    "                    cnt += 1\n",
    "                    continue\n",
    "                ids = self.tokenizer(v)\n",
    "                if len(ids[\"input_ids\"]) != 3:\n",
    "                    print(k, \"   \", v)\n",
    "                if v in exist_value:\n",
    "                    print(\"exist\", k, \"  \", v)\n",
    "                else:\n",
    "                    exist_value.append(v)\n",
    "        elif dataset_name == \"webnlg_star\":\n",
    "            self.pred2text = dataprocess.rel2text.webnlg_rel2text\n",
    "            cnt = 1\n",
    "            exist_value=[]\n",
    "            for k in self.pred2text:\n",
    "                v = self.pred2text[k]\n",
    "                if isinstance(v, int):\n",
    "                    self.pred2text[k] = f\"[unused{cnt}]\" \n",
    "                    cnt += 1\n",
    "                    continue\n",
    "                ids = self.tokenizer(v)\n",
    "                if len(ids[\"input_ids\"]) != 3:\n",
    "                    print(k, \"   \", v)\n",
    "                if v in exist_value:\n",
    "                    print(\"exist\", k, \"  \", v)\n",
    "                else:\n",
    "                    exist_value.append(v)\n",
    "            # self.pred2text = {key: \"[unused\"+str(i+1)+\"]\" for i, key in enumerate(self.label2id.keys())}\n",
    "        else:\n",
    "            print(\"dataset name error\")\n",
    "            exit(0)\n",
    "        self.pred_str = \"\"\n",
    "        self.max_label_len = 1\n",
    "        self.pred2idx = {}\n",
    "        idx = 0\n",
    "        for k in self.pred2text:\n",
    "            self.pred2idx[k] = idx\n",
    "            self.pred_str += self.pred2text[k] + \" \"\n",
    "            idx += 1\n",
    "        self.num_rels = len(self.pred2text.keys())\n",
    "        self.idx2pred = {value: key for key, value in self.pred2idx.items()}\n",
    "        self.pred_str = self.pred_str[:-1]\n",
    "        self.pred_inputs = self.tokenizer.encode_plus(self.pred_str,\n",
    "                                                 add_special_tokens=False)\n",
    "    \n",
    "    def _data_process(self, text):\n",
    "        # text could be a list of sentences or a single sentence\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        inputs = self.tokenizer.batch_encode_plus(text, max_length=self.max_length, padding=\"max_length\", truncation=True)\n",
    "        batched_input_ids = []\n",
    "        batched_attention_mask = []\n",
    "        batched_token_type_ids = []\n",
    "        for b_input_ids, b_attention_mask, b_token_type_ids in zip(inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"token_type_ids\"]):\n",
    "            input_ids = b_input_ids + self.pred_inputs[\"input_ids\"]\n",
    "            sep_idx = b_input_ids.index(self.tokenizer.sep_token_id)\n",
    "            input_ids[sep_idx] = self.tokenizer.pad_token_id\n",
    "            attention_mask = b_attention_mask + [1]*self.num_rels\n",
    "            attention_mask[sep_idx] = 0\n",
    "            token_type_ids = b_token_type_ids + [1]*self.num_rels\n",
    "            batched_input_ids.append(input_ids)\n",
    "            batched_attention_mask.append(attention_mask)\n",
    "            batched_token_type_ids.append(token_type_ids)\n",
    "        return batched_input_ids, batched_attention_mask, batched_token_type_ids\n",
    "    \n",
    "\n",
    "    def _get_e2r(self, e2r_pred):\n",
    "        \"\"\"\n",
    "        Extract entity-relation (subject-relation) and entity-entity interactions from given Attention Matrix.\n",
    "        Only Extract the upper-right triangle, so should input transpose of the original\n",
    "        Attention Matrix to extract relation-entity (relation-object) interactions.\n",
    "        \"\"\"\n",
    "        token_len = self.max_length-2\n",
    "        e2r = {}\n",
    "        tok_tok = set()\n",
    "        e_va = np.where(e2r_pred == 1)\n",
    "        for h, r in zip(e_va[0], e_va[1]):\n",
    "            h = int(h)\n",
    "            r = int(r)            \n",
    "            if h == 0 or r == 0 or r == token_len+1 or h > token_len:\n",
    "                continue\n",
    "            # Entity-Entity\n",
    "            if r < token_len+1:\n",
    "                tok_tok.add((h,r))\n",
    "            # Entity-Relation\n",
    "            else:\n",
    "                r = int(r-token_len-2)\n",
    "                if h not in e2r:\n",
    "                    e2r[h] = []\n",
    "                e2r[h].append(r)\n",
    "        return e2r, tok_tok\n",
    "    \n",
    "    def _get_span_att(self, span_pred):\n",
    "        token_len = self.max_length-2\n",
    "        span_va = np.where(span_pred == 1)\n",
    "        t2_span = dict()\n",
    "        h2_span = dict()\n",
    "        for s, e in zip(span_va[0], span_va[1]):\n",
    "            # if s > token_len or e > token_len or s == 0 or e == 0:\n",
    "            if s > token_len or e > token_len:\n",
    "                continue\n",
    "            if e < s:\n",
    "                continue\n",
    "            if e not in t2_span:\n",
    "                t2_span[e] = []\n",
    "            if s not in h2_span:\n",
    "                h2_span[s] = []\n",
    "            s = int(s)\n",
    "            e = int(e)\n",
    "            t2_span[e].append((s,e))\n",
    "            h2_span[s].append((s,e))\n",
    "        return  h2_span, t2_span\n",
    "\n",
    "    def _extractor(self, outputs, input_ids_list):\n",
    "        preds_list = []\n",
    "        for head_pred, tail_pred, span_pred, input_ids in zip(outputs[\"head_preds\"], outputs[\"tail_preds\"], outputs[\"span_preds\"], input_ids_list):\n",
    "            pred_spo_text = set()\n",
    "            s_h2r, s2s = self._get_e2r(head_pred)\n",
    "            s_t2r, _ = self._get_e2r(head_pred.T)\n",
    "            e_h2r, e2e = self._get_e2r(tail_pred)\n",
    "            e_t2r, _ = self._get_e2r(tail_pred.T)\n",
    "            start2span, end2span = self._get_span_att(span_pred)\n",
    "            for l, r in e2e:\n",
    "                if l not in e_h2r or r not in e_t2r:\n",
    "                    continue\n",
    "                if l not in end2span or r not in end2span:\n",
    "                    continue\n",
    "                l_spans, r_spans = end2span[l], end2span[r]\n",
    "                for l_span in l_spans:\n",
    "                    for r_span in r_spans:\n",
    "                        l_s, r_s = l_span[0], r_span[0]\n",
    "                        if (l_s, r_s) not in s2s:\n",
    "                            continue\n",
    "                        if l_s not in s_h2r or r_s not in s_t2r:\n",
    "                            continue\n",
    "                        common_rels = set(s_h2r[l_s])& set(s_t2r[r_s]) & set(e_h2r[l]) & set(e_t2r[r])\n",
    "                        # l_span_new = (l_span[0]+1, l_span[1])\n",
    "                        # r_span_new = (r_span[0]+1, r_span[1])\n",
    "                        l_span_new = (l_span[0], l_span[1])\n",
    "                        r_span_new = (r_span[0], r_span[1])\n",
    "                        for rel in common_rels:\n",
    "                            pred_spo_text.add((\n",
    "                                self.tokenizer.decode(input_ids[l_span_new[0]:l_span_new[1]+1]),\n",
    "                                self.idx2pred[rel],\n",
    "                                self.tokenizer.decode(input_ids[r_span_new[0]:r_span_new[1]+1])\n",
    "                            ))\n",
    "            preds_list.append(list(pred_spo_text))\n",
    "        return preds_list\n",
    "\n",
    "    def predict(self, text):\n",
    "        input_ids, attention_mask, token_type_ids = self._data_process(text)\n",
    "        if isinstance(input_ids, list):\n",
    "            input_ids = torch.tensor(input_ids)\n",
    "            attention_mask = torch.tensor(attention_mask)\n",
    "            token_type_ids = torch.tensor(token_type_ids)\n",
    "        else:\n",
    "            input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "            attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
    "            token_type_ids = torch.tensor(token_type_ids).unsqueeze(0)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask, token_type_ids)\n",
    "            results = self._extractor(outputs, input_ids)\n",
    "        return results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6dfca5-48e7-47b9-8000-352925bb0a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations_from_model_output(text):\n",
    "    relations = []\n",
    "    unirel = UniRel(\"nyt-checkpoint-final\", dataset_name=\"nyt\")\n",
    "    \n",
    "    # Собираем все предложения в один список\n",
    "    all_sentences = []\n",
    "    for content in text['content']:\n",
    "        all_sentences.extend(sent_tokenize(content))\n",
    "    \n",
    "    # Проходим по каждому предложению и делаем предсказание\n",
    "    for sentence in all_sentences:\n",
    "        result = unirel.predict(sentence)\n",
    "        \n",
    "        # Если есть предсказания, добавляем их в список отношений\n",
    "        if result[0]:\n",
    "            for prediction in result[0]:\n",
    "                relations.append({\n",
    "                    'head': prediction[0].strip(),\n",
    "                    'type': prediction[1].strip(),\n",
    "                    'tail': prediction[2].strip()\n",
    "                })\n",
    "    \n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f40691c8-bb8b-4f1b-bf68-66deae5312ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bbc-news-data.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b65c4-d643-45b9-85b1-ae4db41da30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stas/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:905: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gav = extract_relations_from_model_output(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49087c57-42ae-4e8b-a5b5-5e43abdee467",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(\n",
    "    directed=True,\n",
    "    width=\"700px\",\n",
    "    height=\"700px\",\n",
    "    bgcolor=\"#eeeeee\"\n",
    "    )\n",
    "\n",
    "color_entity = \"#00FF00\"\n",
    "for e in gav:\n",
    "    net.add_node(e[\"head\"], shape=\"circle\", color=color_entity)\n",
    "for e in gav:\n",
    "    net.add_node(e[\"tail\"], shape=\"circle\", color=color_entity)\n",
    "\n",
    "for rel in gav:\n",
    "    net.add_edge(\n",
    "        rel[\"head\"],\n",
    "        rel[\"tail\"],\n",
    "        title=rel[\"type\"],\n",
    "        label=rel[\"type\"]\n",
    "        )\n",
    "        \n",
    "net.repulsion(\n",
    "    node_distance=200,\n",
    "    central_gravity=0.2,\n",
    "    spring_length=200,\n",
    "    spring_strength=0.05,\n",
    "    damping=0.09\n",
    ")\n",
    "net.set_edge_smooth('dynamic')\n",
    "net.show('./test.html', notebook=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8834950-079d-43ba-96e0-dedeb6ff8f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
