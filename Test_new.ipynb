{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import math\n",
    "import torch\n",
    "import wikipedia\n",
    "from pyvis.network import Network\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "from bertopic import BERTopic\n",
    "from nltk.corpus import stopwords\n",
    "# from topic_modelling import dim_reduction_texts_topics\n",
    "\n",
    "# from viz import (\n",
    "#     viz_topic_bubbles,\n",
    "#     viz_scatter_texts,\n",
    "#     viz_word_scores,\n",
    "#     viz_topic_heatmap,\n",
    "#     viz_classes_corpus,\n",
    "#     viz_classes_per_topic,\n",
    "#     viz_ner_per_topic,\n",
    "#     viz_n_grams_per_topic\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "\n",
    "def extract_topics(\n",
    "        texts,\n",
    "        embedder_name=\"all-MiniLM-L6-v2\",\n",
    "        n_neighbors=15,\n",
    "        n_components=10,\n",
    "        umap_metric='cosine',\n",
    "        random_state=42,\n",
    "        min_cluster_size=50,\n",
    "        hdbscan_metric='euclidean'\n",
    "):\n",
    "    embedding_model = SentenceTransformer(embedder_name)\n",
    "    embeddings = embedding_model.encode(texts)\n",
    "\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        n_components=n_components,\n",
    "        min_dist=0.0,\n",
    "        metric=umap_metric,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    hdbscan_model = HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        metric=hdbscan_metric,\n",
    "        cluster_selection_method='eom',\n",
    "        prediction_data=True\n",
    "    )\n",
    "\n",
    "    representation_model = KeyBERTInspired()\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        representation_model=representation_model,\n",
    "        top_n_words=10,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    topics, _ = topic_model.fit_transform(texts, embeddings)\n",
    "\n",
    "    return topics, topic_model, embeddings\n",
    "\n",
    "\n",
    "def dim_reduction_texts_topics(textual_embeddings, topic_embeddings):\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=15,\n",
    "        n_components=2,\n",
    "        min_dist=0.0,\n",
    "        metric='cosine',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    projected_texts = umap_model.fit_transform(textual_embeddings)\n",
    "    projected_topics = umap_model.transform(topic_embeddings)\n",
    "\n",
    "    return projected_texts, projected_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kirillanosov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def join_docs(doc):\n",
    "    return '\\n\\n'.join(doc)\n",
    "\n",
    "\n",
    "def clean_doc(doc):\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub('[^a-z A-Z 0-9-]+', '', doc)\n",
    "    doc = \" \".join([word for word in doc.split() if word not in stopwords])\n",
    "    return doc\n",
    "\n",
    "\n",
    "def viz_topic_bubbles(\n",
    "        topic_model,\n",
    "        projected_topics,\n",
    "        texts\n",
    "        ):\n",
    "\n",
    "    x = projected_topics[:, :1]\n",
    "    y = projected_topics[:, 1:]\n",
    "    topic_freq = topic_model.get_topic_freq()\n",
    "    doc_info = topic_model.get_document_info(texts)\n",
    "    df = topic_freq.merge(doc_info, on='Topic', how='left')\n",
    "    df = df.groupby(['Topic', 'Top_n_words', 'Count', 'Name']).agg({'Probability': 'mean'}).reset_index()\n",
    "    df['x'] = x\n",
    "    df['y'] = y\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x='x',\n",
    "        y='y',\n",
    "        hover_data={\n",
    "            \"Topic\": True,\n",
    "            \"Top_n_words\": True,\n",
    "            \"Count\": True,\n",
    "            \"x\": False,\n",
    "            \"y\": False\n",
    "        },\n",
    "        text='Topic',\n",
    "        size='Count',\n",
    "        color='Name',\n",
    "        size_max=100,\n",
    "        template='plotly_white',\n",
    "    )\n",
    "\n",
    "    fig.update_traces(marker=dict(line=dict(width=1, color='Gray')))\n",
    "\n",
    "    fig.update_xaxes(visible=False)\n",
    "    fig.update_yaxes(visible=False)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def viz_scatter_texts(\n",
    "        topic_model,\n",
    "        texts,\n",
    "        projected_texts\n",
    "        ):\n",
    "\n",
    "    topic_freq = topic_model.get_topic_freq()\n",
    "    doc_info = topic_model.get_document_info(texts)\n",
    "    df = topic_freq.merge(doc_info, on='Topic', how='left')\n",
    "    x = projected_texts[:, :1]\n",
    "    y = projected_texts[:, 1:]\n",
    "    df['x'] = x\n",
    "    df['y'] = y\n",
    "    texts_c = df.groupby(['Topic']).agg({'Document': 'nunique'}).reset_index()\n",
    "    texts_c = texts_c.rename(columns={'Document': 'Document_qty'})\n",
    "    df = df.merge(texts_c, on='Topic', how='left')\n",
    "    df.Document = df.Document.apply(lambda x: x[:100] + '...')\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x='x',\n",
    "        y='y',\n",
    "        hover_data={\n",
    "            \"Topic\": False,\n",
    "            \"Name\": True,\n",
    "            \"Document\": False,\n",
    "            \"Document_qty\": False,\n",
    "            \"x\": False,\n",
    "            \"y\": False\n",
    "        },\n",
    "        hover_name='Document',\n",
    "        color='Name',\n",
    "        size_max=60,\n",
    "        template='plotly_white',\n",
    "    )\n",
    "\n",
    "    fig.update_traces(marker=dict(line=dict(width=1, color='Gray')))\n",
    "    fig.update_xaxes(visible=False)\n",
    "    fig.update_yaxes(visible=False)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def viz_word_scores(\n",
    "        topic_model,\n",
    "        top_n_topics=8,\n",
    "        n_words=5,\n",
    "        custom_labels=False,\n",
    "        title=\"<b>Вероятности слов по темам</b>\",\n",
    "        width=250,\n",
    "        height=250\n",
    "):\n",
    "\n",
    "    colors = itertools.cycle([\"#D55E00\", \"#0072B2\", \"#CC79A7\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\"])\n",
    "\n",
    "    freq_df = topic_model.get_topic_freq()\n",
    "    freq_df = freq_df.loc[freq_df.Topic != -1, :]\n",
    "    if top_n_topics is not None:\n",
    "        topics = sorted(freq_df.Topic.to_list()[:top_n_topics])\n",
    "    else:\n",
    "        topics = sorted(freq_df.Topic.to_list()[0:6])\n",
    "\n",
    "    if isinstance(custom_labels, str):\n",
    "        subplot_titles = [[[str(topic), None]] + topic_model.topic_aspects_[custom_labels][topic] for topic in topics]\n",
    "        subplot_titles = [\"_\".join([label[0] for label in labels[:4]]) for labels in subplot_titles]\n",
    "        subplot_titles = [label if len(label) < 30 else label[:27] + \"...\" for label in subplot_titles]\n",
    "    elif topic_model.custom_labels_ is not None and custom_labels:\n",
    "        subplot_titles = [topic_model.custom_labels_[topic + topic_model._outliers] for topic in topics]\n",
    "    else:\n",
    "        subplot_titles = [f\"Тема {topic}\" for topic in topics]\n",
    "    columns = 4\n",
    "    rows = int(np.ceil(len(topics) / columns))\n",
    "    fig = make_subplots(\n",
    "        rows=rows,\n",
    "        cols=columns,\n",
    "        shared_xaxes=False,\n",
    "        horizontal_spacing=.1,\n",
    "        vertical_spacing=.4 / rows if rows > 1 else 0,\n",
    "        subplot_titles=subplot_titles\n",
    "    )\n",
    "\n",
    "    row = 1\n",
    "    column = 1\n",
    "    for topic in topics:\n",
    "        words = [word + \"  \" for word, _ in topic_model.get_topic(topic)][:n_words][::-1]\n",
    "        scores = [score for _, score in topic_model.get_topic(topic)][:n_words][::-1]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=scores,\n",
    "                   y=words,\n",
    "                   orientation='h',\n",
    "                   marker_color=next(colors)),\n",
    "            row=row, col=column)\n",
    "\n",
    "        if column == columns:\n",
    "            column = 1\n",
    "            row += 1\n",
    "        else:\n",
    "            column += 1\n",
    "\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        showlegend=False,\n",
    "        title={\n",
    "            'text': f\"{title}\",\n",
    "            'x': .5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top',\n",
    "            'font': dict(\n",
    "                size=22,\n",
    "                color=\"Black\")\n",
    "        },\n",
    "        width=width * 4,\n",
    "        height=height * rows if rows > 1 else height * 1.3,\n",
    "        hoverlabel=dict(\n",
    "            bgcolor=\"white\",\n",
    "            font_size=16,\n",
    "            font_family=\"Rockwell\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(showgrid=True)\n",
    "    fig.update_yaxes(showgrid=True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def viz_topic_heatmap(\n",
    "        topic_model,\n",
    "        topics=None,\n",
    "        top_n_topics=None,\n",
    "        n_clusters=None,\n",
    "        custom_labels=False,\n",
    "        title=\"<b>Матрица семантической близости тем</b>\",\n",
    "        width=800,\n",
    "        height=800\n",
    "):\n",
    "\n",
    "    if topic_model.topic_embeddings_ is not None:\n",
    "        embeddings = np.array(topic_model.topic_embeddings_)[topic_model._outliers:]\n",
    "    else:\n",
    "        embeddings = topic_model.c_tf_idf_[topic_model._outliers:]\n",
    "\n",
    "    freq_df = topic_model.get_topic_freq()\n",
    "    freq_df = freq_df.loc[freq_df.Topic != -1, :]\n",
    "\n",
    "    if top_n_topics is not None:\n",
    "        topics = sorted(freq_df.Topic.to_list()[:top_n_topics])\n",
    "    else:\n",
    "        topics = sorted(freq_df.Topic.to_list())\n",
    "\n",
    "    sorted_topics = topics\n",
    "\n",
    "    if n_clusters:\n",
    "        distance_matrix = cosine_similarity(embeddings[topics])\n",
    "        Z = linkage(distance_matrix, 'ward')\n",
    "        clusters = fcluster(Z, t=n_clusters, criterion='maxclust')\n",
    "\n",
    "        mapping = {cluster: [] for cluster in clusters}\n",
    "        for topic, cluster in zip(topics, clusters):\n",
    "            mapping[cluster].append(topic)\n",
    "        mapping = [cluster for cluster in mapping.values()]\n",
    "        sorted_topics = [topic for cluster in mapping for topic in cluster]\n",
    "\n",
    "    indices = np.array([topics.index(topic) for topic in sorted_topics])\n",
    "    embeddings = embeddings[indices]\n",
    "    distance_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    if isinstance(custom_labels, str):\n",
    "        new_labels = [[[str(topic), None]] + topic_model.topic_aspects_[custom_labels][topic] for topic in\n",
    "                      sorted_topics]\n",
    "        new_labels = [\"_\".join([label[0] for label in labels[:4]]) for labels in new_labels]\n",
    "        new_labels = [label if len(label) < 30 else label[:27] + \"...\" for label in new_labels]\n",
    "    elif topic_model.custom_labels_ is not None and custom_labels:\n",
    "        new_labels = [topic_model.custom_labels_[topic + topic_model._outliers] for topic in sorted_topics]\n",
    "    else:\n",
    "        new_labels = [[[str(topic), None]] + topic_model.get_topic(topic) for topic in sorted_topics]\n",
    "        new_labels = [\"_\".join([label[0] for label in labels[:4]]) for labels in new_labels]\n",
    "        new_labels = [label if len(label) < 30 else label[:27] + \"...\" for label in new_labels]\n",
    "\n",
    "    fig = px.imshow(\n",
    "        distance_matrix,\n",
    "        labels=dict(color=\"Оценка близости\"),\n",
    "        x=new_labels,\n",
    "        y=new_labels,\n",
    "        color_continuous_scale='GnBu'\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': f\"{title}\",\n",
    "            'y': .95,\n",
    "            'x': 0.55,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top',\n",
    "            'font': dict(\n",
    "                size=22,\n",
    "                color=\"Black\"\n",
    "            )\n",
    "        },\n",
    "        width=width,\n",
    "        height=height,\n",
    "        hoverlabel=dict(\n",
    "            bgcolor=\"white\",\n",
    "            font_size=16,\n",
    "            font_family=\"Rockwell\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_layout(showlegend=True)\n",
    "    fig.update_layout(legend_title_text='Trend')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def viz_classes_corpus(classes):\n",
    "    df = pd.DataFrame({'classes': classes})\n",
    "    df = df.value_counts().rename_axis('classes').reset_index(name='counts')\n",
    "    fig = px.bar(df, x='classes', y='counts', color='classes')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def viz_classes_per_topic(classes, topics,topic=1):\n",
    "\n",
    "    df = pd.DataFrame({'classes': classes, 'topics': topics})\n",
    "    df = df[df['topics'] == topic].drop(['topics'], axis=1)\n",
    "    df = df.value_counts().rename_axis('classes').reset_index(name='counts')\n",
    "    fig = px.bar(df, x='classes', y='counts', color='classes')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def viz_ner_per_topic(ents, ner_topics, topic=1):\n",
    "    df = pd.DataFrame({'ents': ents, 'topics': ner_topics})\n",
    "    df = df[df['topics'] == topic]\n",
    "    df.drop(['topics'], inplace=True, axis=1)\n",
    "    df['ents'] = df['ents'].apply(lambda x: x.strip())\n",
    "    df = df.value_counts().rename_axis('entity').reset_index(name='counts').head(10)\n",
    "    fig = px.bar(df, x='entity', y='counts')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def viz_n_grams_per_topic(texts, topic_model, topic=1, n=3):\n",
    "    ngram_freq_df = pd.DataFrame()\n",
    "    vectorizer = CountVectorizer(ngram_range=(n,n))\n",
    "    df = topic_model.get_document_info(texts)\n",
    "    df = df[df['Topic'] == topic]\n",
    "    df['Document'] = df['Document'].apply(clean_doc)\n",
    "\n",
    "    ngrams = vectorizer.fit_transform(df['Document'])\n",
    "    count_values = ngrams.toarray().sum(axis=0)\n",
    "    ngram_freq = pd.DataFrame(\n",
    "        sorted([(count_values[i], k) for k, i in vectorizer.vocabulary_.items()],\n",
    "        reverse=True),\n",
    "        columns=[\"частота\", \"n-gram\"]\n",
    "        )\n",
    "\n",
    "    ngram_freq_df = pd.concat([ngram_freq_df, ngram_freq])\n",
    "    top_ngram = ngram_freq_df.sort_values(by='частота', ascending=False).head(10)\n",
    "\n",
    "    fig = px.bar(\n",
    "        top_ngram,\n",
    "        x='частота',\n",
    "        y='n-gram',\n",
    "        orientation='h',\n",
    "        title=f'Top-10 {n}-грамм для темы \"{df.Name.iloc[0]}\"'\n",
    "        )\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations_from_model_output(text):\n",
    "    relations = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
    "    for token in text_replaced.split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        relations.append({\n",
    "            'head': subject.strip(),\n",
    "            'type': relation.strip(),\n",
    "            'tail': object_.strip()\n",
    "        })\n",
    "    return relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KB():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.entities = {}\n",
    "        self.relations = []\n",
    "\n",
    "\n",
    "    def merge_with_kb(self, kb2):\n",
    "        for rel in kb2.relations:\n",
    "            self.add_relation(rel)\n",
    "\n",
    "\n",
    "    def are_relations_equal(self, rel1, rel2):\n",
    "        return all(rel1[attr] == rel2[attr] for attr in [\"head\", \"type\", \"tail\"])\n",
    "\n",
    "\n",
    "    def exists_relation(self, rel1):\n",
    "        return any(self.are_relations_equal(rel1, rel2) for rel2 in self.relations)\n",
    "\n",
    "\n",
    "    def merge_relations(self, rel2):\n",
    "        rel1 = [rel for rel in self.relations if self.are_relations_equal(rel2, rel)][0]\n",
    "        text = list(rel2[\"meta\"].keys())[0]\n",
    "\n",
    "        if text not in rel1[\"meta\"]:\n",
    "            rel1[\"meta\"][text] = rel2[\"meta\"][text]\n",
    "        else:\n",
    "            spans_to_add = [span for span in rel2[\"meta\"][text][\"spans\"] if span not in rel1[\"meta\"][text][\"spans\"]]\n",
    "            rel1[\"meta\"][text][\"spans\"] += spans_to_add\n",
    "\n",
    "\n",
    "    def get_wikipedia_data(self, candidate_entity):\n",
    "        try:\n",
    "            page = wikipedia.page(candidate_entity, auto_suggest=False)\n",
    "            entity_data = {\n",
    "                \"title\": page.title,\n",
    "                \"url\": page.url,\n",
    "                \"summary\": page.summary\n",
    "            }\n",
    "            return entity_data\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def add_entity(self, ent):\n",
    "        self.entities[ent[\"title\"]] = {k:v for k,v in ent.items() if k != \"title\"}\n",
    "\n",
    "\n",
    "    def add_relation(self, rel):\n",
    "        candidate_entities = [rel[\"head\"], rel[\"tail\"]]\n",
    "        entities = [self.get_wikipedia_data(ent) for ent in candidate_entities]\n",
    "\n",
    "        if any(ent is None for ent in entities):\n",
    "            return\n",
    "\n",
    "        for ent in entities:\n",
    "            self.add_entity(ent)\n",
    "\n",
    "        rel[\"head\"] = entities[0][\"title\"]\n",
    "        rel[\"tail\"] = entities[1][\"title\"]\n",
    "\n",
    "        if not self.exists_relation(rel):\n",
    "            self.relations.append(rel)\n",
    "        else:\n",
    "            self.merge_relations(rel)\n",
    "\n",
    "\n",
    "    def print(self):\n",
    "        print(\"Entities:\")\n",
    "        for ent in self.entities.items():\n",
    "            print(f\"  {ent}\")\n",
    "\n",
    "        print(\"Relations:\")\n",
    "        for rel in self.relations:\n",
    "            print(f\"  {rel}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_text_to_kb(text, span_length=128):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "\n",
    "    num_tokens = len(inputs[\"input_ids\"][0])\n",
    "    num_spans = math.ceil(num_tokens / span_length)\n",
    "    overlap = math.ceil((num_spans * span_length - num_tokens) / max(num_spans - 1, 1))\n",
    "\n",
    "    spans_boundaries = []\n",
    "    start = 0\n",
    "\n",
    "    for i in range(num_spans):\n",
    "        spans_boundaries.append([start + span_length * i, start + span_length * (i + 1)])\n",
    "        start -= overlap\n",
    "\n",
    "    tensor_ids = [\n",
    "        inputs[\"input_ids\"][0][boundary[0]:boundary[1]] for boundary in spans_boundaries\n",
    "        ]\n",
    "    tensor_masks = [\n",
    "        inputs[\"attention_mask\"][0][boundary[0]:boundary[1]] for boundary in spans_boundaries\n",
    "        ]\n",
    "    inputs = {\n",
    "        \"input_ids\": torch.stack(tensor_ids),\n",
    "        \"attention_mask\": torch.stack(tensor_masks)\n",
    "        }\n",
    "    num_return_sequences = 3\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": 256,\n",
    "        \"length_penalty\": 0,\n",
    "        \"num_beams\": 3,\n",
    "        \"num_return_sequences\": num_return_sequences\n",
    "        }\n",
    "\n",
    "    generated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        **gen_kwargs,\n",
    "        )\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(\n",
    "        generated_tokens,\n",
    "        skip_special_tokens=False\n",
    "        )\n",
    "\n",
    "    kb = KB()\n",
    "\n",
    "    idx = 0\n",
    "    for sentence_pred in decoded_preds:\n",
    "        current_span_index = idx // num_return_sequences\n",
    "        relations = extract_relations_from_model_output(sentence_pred)\n",
    "        for relation in relations:\n",
    "            relation[\"meta\"] = {\n",
    "                text: {\n",
    "                    \"spans\": [spans_boundaries[current_span_index]]\n",
    "                }\n",
    "            }\n",
    "            kb.add_relation(relation)\n",
    "        idx += 1\n",
    "\n",
    "    return kb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_corpus_to_kb(corpus):\n",
    "    kb = KB()\n",
    "    for text in tqdm(corpus):\n",
    "        try:\n",
    "            kb_text = from_text_to_kb(text)\n",
    "            kb.merge_with_kb(kb_text)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return kb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(pd.read_csv('/Users/kirillanosov/Downloads/serendipity/data/bbc-news-data.csv', sep='\\t')['content'].apply(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4863620e5f4f3984092fccb5514a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 s, sys: 3.2 s, total: 23.4 s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kb = from_corpus_to_kb(X[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.KB at 0x106381b10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio\n",
    "print(gradio.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./100.html\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "net = Network(\n",
    "    directed=True,\n",
    "    width=\"700px\",\n",
    "    height=\"700px\",\n",
    "    bgcolor=\"#eeeeee\"\n",
    "    )\n",
    "\n",
    "color_entity = \"#00FF00\"\n",
    "for e in kb.entities:\n",
    "    net.add_node(e, shape=\"circle\", color=color_entity)\n",
    "\n",
    "for rel in kb.relations:\n",
    "    net.add_edge(\n",
    "        rel[\"head\"],\n",
    "        rel[\"tail\"],\n",
    "        title=rel[\"type\"],\n",
    "        label=rel[\"type\"]\n",
    "        )\n",
    "\n",
    "net.repulsion(\n",
    "    node_distance=200,\n",
    "    central_gravity=0.2,\n",
    "    spring_length=200,\n",
    "    spring_strength=0.05,\n",
    "    damping=0.09\n",
    ")\n",
    "net.set_edge_smooth('dynamic')\n",
    "net.show('./100.html', notebook=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Running on public URL: https://d22c2e2b07c5f58076.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d22c2e2b07c5f58076.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from pyvis.network import Network\n",
    "\n",
    "def generate_html(kb_entities, kb_relations):\n",
    "    net = Network(\n",
    "        directed=True,\n",
    "        width=\"700px\",\n",
    "        height=\"700px\",\n",
    "        bgcolor=\"#eeeeee\"\n",
    "    )\n",
    "\n",
    "    color_entity = \"#00FF00\"\n",
    "    for e in kb.entities:\n",
    "        net.add_node(e, shape=\"circle\", color=color_entity)\n",
    "\n",
    "    for rel in kb.relations:\n",
    "        net.add_edge(\n",
    "            rel[\"head\"],\n",
    "            rel[\"tail\"],\n",
    "            title=rel[\"type\"],\n",
    "            label=rel[\"type\"]\n",
    "        )\n",
    "\n",
    "    net.repulsion(\n",
    "        node_distance=200,\n",
    "        central_gravity=0.2,\n",
    "        spring_length=200,\n",
    "        spring_strength=0.05,\n",
    "        damping=0.09\n",
    "    )\n",
    "\n",
    "    net.set_edge_smooth('dynamic')\n",
    "    html = net.generate_html()\n",
    "    html = html.replace(\"'\", \"\\\"\")\n",
    "    iframe_html = f\"\"\"<iframe style=\"width: 100%; height: 600px;margin:0 auto\" name=\"result\" allow=\"midi; geolocation; microphone; camera; display-capture; encrypted-media;\" sandbox=\"allow-modals allow-forms allow-scripts allow-same-origin allow-popups allow-top-navigation-by-user-activation allow-downloads\" allowfullscreen=\"\" allowpaymentrequest=\"\" frameborder=\"0\" srcdoc='{html}'></iframe>\"\"\"\n",
    "    return iframe_html\n",
    "\n",
    "# Define your Gradio interface with direct component imports\n",
    "demo = gr.Interface(\n",
    "    fn=generate_html,\n",
    "    inputs=[\n",
    "        gr.Dataframe(headers=[\"Entities\"]), \n",
    "        gr.Dataframe(headers=[\"Head\", \"Tail\", \"Type\"])\n",
    "    ],\n",
    "    outputs=\"html\",\n",
    "    title=\"Network Visualization with Pyvis in Gradio\",\n",
    "    allow_flagging='never'\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio.outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutputs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moutputs\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio.outputs'"
     ]
    }
   ],
   "source": [
    "import gradio.outputs as outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda info --envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/kirillanosov/opt/anaconda3/lib/python3.8/site-packages/gradio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
